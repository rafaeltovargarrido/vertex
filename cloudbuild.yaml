steps:
  # 1. Construir
  - name: 'gcr.io/cloud-builders/docker'
    args: [
      'build', 
      '-t', 'europe-docker.pkg.dev/$PROJECT_ID/${_REPO_NAME}/training-image:latest', 
      '.'
    ]

  # 2. Subir
  - name: 'gcr.io/cloud-builders/docker'
    args: [
      'push', 
      'europe-docker.pkg.dev/$PROJECT_ID/${_REPO_NAME}/training-image:latest'
    ]


# -------------------------------------------------------------------------
  # 3. Lanzar entrenamiento Y ESPERAR A QUE TERMINE (SÃ­ncrono)
  # -------------------------------------------------------------------------
  - name: 'gcr.io/google.com/cloudsdktool/cloud-sdk'
    entrypoint: /bin/bash
    args:
      - -c
      - |
        # Usamos $$ para que Cloud Build ignore esta variable y la deje para Bash
        JOB_ID=$$(gcloud ai custom-jobs create \
          --region=${_REGION} \
          --display-name=training-job-$BUILD_ID \
          --worker-pool-spec=machine-type=n1-standard-4,replica-count=1,container-image-uri=europe-docker.pkg.dev/$PROJECT_ID/${_REPO_NAME}/training-image:$COMMIT_SHA \
          --args=--model_dir=gs://${_BUCKET}/model_output/ \
          --format="value(name)")
        
        echo "Trabajo de entrenamiento iniciado: $$JOB_ID"
        
        # Esperar a que termine usando la variable de bash $$JOB_ID
        gcloud ai custom-jobs stream-logs $$JOB_ID --region=${_REGION}

  # 4. Desplegar
  - name: 'python:3.9-slim'
    entrypoint: /bin/sh
    args:
      - -c
      - |
        pip install google-cloud-aiplatform
        python deploy.py \
          --project_id=$PROJECT_ID \
          --region=${_REGION} \
          --bucket=${_BUCKET} \
          --name=mi-modelo-regresion

images:
  - 'europe-docker.pkg.dev/$PROJECT_ID/${_REPO_NAME}/training-image:latest'

substitutions:
    _REGION: europe-west4
    _BUCKET: dataflow_vertex
    _REPO_NAME: images