steps:
  # 1. Construir la imagen Docker
  - name: 'gcr.io/cloud-builders/docker'
    args: [
      'build', 
      '-t', 'europe-docker.pkg.dev/$PROJECT_ID/${_REPO_NAME}/training-image:latest', 
      '.'
    ]

  # 2. Subir la imagen a Artifact Registry
  - name: 'gcr.io/cloud-builders/docker'
    args: [
      'push', 
      'europe-docker.pkg.dev/$PROJECT_ID/${_REPO_NAME}/training-image:latest'
    ]

  # 3. Lanzar entrenamiento (CORREGIDO)
  # Hemos movido la definición del bucket DENTRO de worker-pool-spec usando 'env'
  - name: 'gcr.io/google.com/cloudsdktool/cloud-sdk'
    entrypoint: gcloud
    args:
      - ai
      - custom-jobs
      - create
      - --region=${_REGION}
      - --display-name=training-job-$BUILD_ID
      # Fíjate en el final de esta línea: env=AIP_MODEL_DIR=...
      - --worker-pool-spec=machine-type=n1-standard-4,replica-count=1,container-image-uri=europe-docker.pkg.dev/$PROJECT_ID/${_REPO_NAME}/training-image:latest,env=AIP_MODEL_DIR=gs://${_BUCKET}/model_output/

  # 4. Desplegar el modelo
  - name: 'python:3.9-slim'
    entrypoint: /bin/sh
    args:
      - -c
      - |
        pip install google-cloud-aiplatform
        python deploy.py \
          --project_id=$PROJECT_ID \
          --region=${_REGION} \
          --bucket=${_BUCKET} \
          --name=mi-modelo-regresion

images:
  - 'europe-docker.pkg.dev/$PROJECT_ID/${_REPO_NAME}/training-image:latest'

substitutions:
    _REGION: europe-west4
    _BUCKET: dataflow_vertex
    _REPO_NAME: images